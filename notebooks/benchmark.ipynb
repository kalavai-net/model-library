{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark a Model\n",
    "\n",
    "A Test harness for benchmarking architectures with different deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/kalavai/lib/python3.11/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_card\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/kalavai/lib/python3.11/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_deployment_template\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model_library.cards.architectures import *\n",
    "from model_library.cards.models import *\n",
    "from model_library.cards.deployments import *\n",
    "from model_library.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = LLAMA_CPP\n",
    "MODEL = LLAMA_CPP_LLAMA_2_7b_CHAT_Q2_GGUF\n",
    "DEPLOYMENT = MINIMAL_CPU_DEPLOYMENT\n",
    "USER = UserInformation(\n",
    "    id=\"benchmark\",\n",
    "    namespace=\"benchmark\",\n",
    "    API_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/model/llama-2-7b-chat.Q2_K.gguf',\n",
       " 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF',\n",
       " 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf',\n",
       " 'volume_name': 'awesome-model-storage',\n",
       " 'pvc_name': 'awesome-model-pvc'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"benchmarkmodel\"\n",
    "\n",
    "template = ModelDeploymentTemplateCard(\n",
    "    id=\"1\",\n",
    "    name=\"\",\n",
    "    description=\"\",\n",
    "    model_card=MODEL,\n",
    "    deployment_card=DEPLOYMENT,\n",
    "    params={\n",
    "        \"deployment_name\": deployment_name\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the models\n",
    "def deploy_generic_model(config):\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # Prepare the request data\n",
    "    data = {\"config\": config}\n",
    "\n",
    "    # Define the URL and headers\n",
    "    url = \"http://0.0.0.0:8000/v1/deploy_generic_model\"\n",
    "    headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def delete_deployment(namespace, deployment_name):\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # Prepare the request data\n",
    "    data = {\"namespace\": namespace, \"deployment_name\": deployment_name}\n",
    "\n",
    "    # Define the URL and headers\n",
    "    url = \"http://0.0.0.0:8000/v1/delete_deepsparse_model\"\n",
    "    headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Delete request successful.\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Delete request failed with status code {response.status_code}.\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def list_deployments(namespace):\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    # Prepare the request data\n",
    "    data = {\"namespace\": namespace}\n",
    "\n",
    "    # Define the URL and headers\n",
    "    url = \"http://0.0.0.0:8000/v1/list_deepsparse_deployments\"\n",
    "    headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request successful.\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy_generic_model(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployments = list_deployments(namespace=USER.namespace)\n",
    "#deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployments = list_deployments(namespace=USER.namespace)\n",
    "#deployments\n",
    "#deployment = deployments[deployment_name]\n",
    "#port = max(deployment[\"ports\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from model_library.benchmark_apis import benchmark_model\n",
    "\n",
    "def benchmark_deployment_template(deployment_template, public_url = \"178.62.13.8\", live_service_cutout=5*60, user = None):\n",
    "\n",
    "\n",
    "    deployment_name = deployment_template.params[\"deployment_name\"]\n",
    "\n",
    "    if user is None:\n",
    "        user = UserInformation(\n",
    "            id=\"benchmark\",\n",
    "            namespace=\"benchmark\",\n",
    "            API_key=\"\"\n",
    "        )\n",
    "\n",
    "    deployment = ModelDeploymentCard(\n",
    "        user_information=USER,\n",
    "        model_deployment_template=deployment_template\n",
    "    )\n",
    "\n",
    "    config = deployment.extract_deployment_config()\n",
    "\n",
    "    deploy_generic_model(config)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    deployments = list_deployments(namespace=USER.namespace)\n",
    "    deployments\n",
    "    deployment = deployments[deployment_name]\n",
    "    port = max(deployment[\"ports\"][0])\n",
    "\n",
    "    print(\"Wait for live service\")\n",
    "\n",
    "\n",
    "    api_available = ARCH.check_health(\n",
    "            port = port,\n",
    "            url = \"178.62.13.8\"\n",
    "        )\n",
    "    warmup_time = 0\n",
    "    interval = 10\n",
    "    \n",
    "    while not api_available and warmup_time < live_service_cutout:\n",
    "        print(\"Waiting \", warmup_time, \" seconds\")\n",
    "        warmup_time+=interval\n",
    "        time.sleep(interval)\n",
    "        api_available = ARCH.check_health(\n",
    "            port = port,\n",
    "            url = \"178.62.13.8\"\n",
    "        )\n",
    "\n",
    "    if not api_available:\n",
    "        return {\n",
    "            \"benchmarks\": {},\n",
    "            \"warmup_time\": warmup_time,\n",
    "            \"template\": deployment_template\n",
    "\n",
    "        }\n",
    "\n",
    "    # TODO SUPPORT DEEPSPARSE\n",
    "    backend_to_test = 'LLAMACPP'\n",
    "    benchmarks = benchmark_model(backend_to_test, port=port, url=public_url)\n",
    "\n",
    "\n",
    "    delete_deployment(user.namespace,deployment_name)\n",
    "\n",
    "    return {\n",
    "            \"benchmarks\": benchmarks.to_dict(\"records\"),\n",
    "            \"warmup_time\": warmup_time,\n",
    "            \"template\": deployment_template,\n",
    "            \"deployment\": deployment\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from model_library.benchmark_apis import benchmark_model\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class KubernetesBenchmark:\n",
    "    def __init__(self, \n",
    "                 deployment_template: Any, \n",
    "                 deploy:bool = True,\n",
    "                 undeploy:bool = True,\n",
    "                 public_url: str = \"178.62.13.8\", \n",
    "                 live_service_cutout: int = 10 * 60, \n",
    "                 user: Optional[Any] = None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the KubernetesBenchmark class.\n",
    "\n",
    "        :param deployment_template: The deployment template object.\n",
    "        :param public_url: Public URL for the deployment.\n",
    "        :param live_service_cutout: Timeout for the service to become live.\n",
    "        :param user: User information object.\n",
    "        :param deploy: Flag to deploy before testing.\n",
    "        :param undeploy: Flag to delete deployment after testing.\n",
    "        \"\"\"\n",
    "        self.deployment_template = deployment_template\n",
    "        self.public_url = public_url\n",
    "        self.live_service_cutout = live_service_cutout\n",
    "        self.user = user or UserInformation(id=\"benchmark\", namespace=\"benchmark\", API_key=\"\")\n",
    "        self.deployment_name = deployment_template.params[\"deployment_name\"]\n",
    "        self.port = None\n",
    "        self.backend_to_test = 'LLAMACPP'  # TODO: Make this configurable if needed\n",
    "        self.undeploy = undeploy\n",
    "        self.deploy_first = deploy\n",
    "        self.architecture = self.deployment_template.model_card.architecture\n",
    "\n",
    "    def deploy(self) -> None:\n",
    "        \"\"\"\n",
    "        Deploy the model.\n",
    "        \"\"\"\n",
    "        if not self.deploy_first:\n",
    "            if not self.architecture.check_health(port=self.port, url=self.public_url):\n",
    "                raise Exception(\"Cannot test an unavailable model.\")\n",
    "\n",
    "        else:\n",
    "            delete_deployment(self.user.namespace, self.deployment_name)\n",
    "\n",
    "\n",
    "        logging.info(\"Deploying the model.\")\n",
    "        deployment = ModelDeploymentCard(\n",
    "            user_information=self.user,\n",
    "            model_deployment_template=self.deployment_template\n",
    "        )\n",
    "        config = deployment.extract_deployment_config()\n",
    "        deploy_generic_model(config)\n",
    "\n",
    "        time.sleep(5)  # Wait for the deployment to initialize\n",
    "        logging.info(\"Model deployed.\")\n",
    "\n",
    "    def wait_for_service(self) -> int:\n",
    "        \"\"\"\n",
    "        Wait for the service to become live.\n",
    "\n",
    "        :return: The time waited for the service to become live.\n",
    "        \"\"\"\n",
    "        logging.info(\"Waiting for the live service.\")\n",
    "        deployments = list_deployments(namespace=self.user.namespace)\n",
    "        deployment = deployments[self.deployment_name]\n",
    "        self.port = max(deployment[\"ports\"][0])\n",
    "\n",
    "        warmup_time = 0\n",
    "        interval = 10\n",
    "\n",
    "        with tqdm(total=self.live_service_cutout) as pbar:\n",
    "            while warmup_time < self.live_service_cutout:\n",
    "                if self.architecture.check_health(port=self.port, url=self.public_url):\n",
    "                    break\n",
    "                time.sleep(interval)\n",
    "                warmup_time += interval\n",
    "                pbar.update(interval)\n",
    "\n",
    "        logging.info(f\"Service is live. Warmup time: {warmup_time} seconds.\")\n",
    "        return warmup_time\n",
    "\n",
    "    def run_benchmark(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the benchmark.\n",
    "\n",
    "        :return: Dictionary of benchmark results.\n",
    "        \"\"\"\n",
    "        logging.info(\"Running benchmarks.\")\n",
    "        benchmarks = benchmark_model(self.backend_to_test, port=self.port, url=self.public_url)\n",
    "        return benchmarks.to_dict(\"records\")\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"\n",
    "        Clean up after benchmarking.\n",
    "        \"\"\"\n",
    "        logging.info(\"Cleaning up the deployment.\")\n",
    "        if self.undeploy:\n",
    "            delete_deployment(self.user.namespace, self.deployment_name)\n",
    "\n",
    "    def benchmark_deployment(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform the entire benchmarking process.\n",
    "\n",
    "        :return: A dictionary containing benchmarking results and metadata.\n",
    "        \"\"\"\n",
    "        self.deploy()\n",
    "        warmup_time = self.wait_for_service()\n",
    "\n",
    "        if warmup_time >= self.live_service_cutout:\n",
    "            logging.warning(\"Service did not become live within the cutoff time.\")\n",
    "            self.cleanup()\n",
    "\n",
    "            return {\n",
    "                \"benchmarks\": {},\n",
    "                \"warmup_time\": warmup_time,\n",
    "                \"template\": self.deployment_template,\n",
    "                \"error\": \"Cut off start time limit reached.\"\n",
    "            }\n",
    "    \n",
    "        try:\n",
    "            benchmarks = self.run_benchmark()\n",
    "        except Exception as e:\n",
    "\n",
    "            return {\n",
    "                    \"benchmarks\": {},\n",
    "                    \"warmup_time\": warmup_time,\n",
    "                    \"template\": self.deployment_template,\n",
    "                    \"error\": f\"Failed to benchmark. {e}\"\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"benchmarks\": benchmarks,\n",
    "            \"warmup_time\": warmup_time,\n",
    "            \"template\": self.deployment_template,\n",
    "            \"deployment\": self.deployment_name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#benchmark = KubernetesBenchmark(template, undeploy=False)\n",
    "#result = benchmark.benchmark_deployment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DeploymentCard(\n",
    "    id=\"4\",\n",
    "    description=\"Minimal Deployment for llama.cpp\",\n",
    "    params={\n",
    "        \"replicas\": 1,\n",
    "        \"num_cpus\": 4,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\",\n",
    "    },\n",
    ")\n",
    "\n",
    "num_cpus = [4,8,12,16]\n",
    "deployments = [\n",
    "    DeploymentCard(\n",
    "    id=\"4\",\n",
    "    description=\"Minimal Deployment for llama.cpp\",\n",
    "    params={\n",
    "        \"replicas\": 2,\n",
    "        \"num_cpus\": n,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\",\n",
    "    }\n",
    "    )\n",
    "    for n in num_cpus\n",
    "]\n",
    "\n",
    "deployment_name = \"benchmarkmodel\"\n",
    "\n",
    "templates = [\n",
    "    ModelDeploymentTemplateCard(\n",
    "        id=\"1\",\n",
    "        name=\"\",\n",
    "        description=\"\",\n",
    "        model_card=MODEL,\n",
    "        deployment_card=deployment,\n",
    "        params={\n",
    "            \"deployment_name\": deployment_name\n",
    "        }\n",
    "    )\n",
    "    for deployment in deployments\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:19,920 - INFO - Deploying the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:25,485 - INFO - Model deployed.\n",
      "2024-01-15 17:19:25,486 - INFO - Waiting for the live service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]\n",
      "2024-01-15 17:19:25,775 - INFO - Service is live. Warmup time: 0 seconds.\n",
      "2024-01-15 17:19:25,776 - INFO - Running benchmarks.\n",
      " 20%|██        | 5/25 [00:24<01:37,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:50,380 - INFO - Deploying the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:56,227 - INFO - Model deployed.\n",
      "2024-01-15 17:19:56,231 - INFO - Waiting for the live service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [10:02<00:00,  1.00s/it]\n",
      "2024-01-15 17:29:59,021 - INFO - Service is live. Warmup time: 600 seconds.\n",
      "2024-01-15 17:29:59,022 - WARNING - Service did not become live within the cutoff time.\n",
      "2024-01-15 17:29:59,022 - INFO - Cleaning up the deployment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for n, template in zip(num_cpus[2:], templates[2:]):\n",
    "    print(\"N: \", n)\n",
    "    benchmark = KubernetesBenchmark(template, undeploy=True)\n",
    "    result = benchmark.benchmark_deployment()\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'benchmarks': [{'type': 'serial',\n",
       "    'average_response_time': 15.530141487121583,\n",
       "    'average_tokens_returned': 57.96,\n",
       "    'total_tokens_returned': 1449,\n",
       "    'wall_time': 388.30193424224854,\n",
       "    'tps': 3.7316321970624475}],\n",
       "  'warmup_time': 360,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 4, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'deployment': 'benchmarkmodel'},\n",
       " {'benchmarks': [{'type': 'serial',\n",
       "    'average_response_time': 5.455150327682495,\n",
       "    'average_tokens_returned': 57.68,\n",
       "    'total_tokens_returned': 1442,\n",
       "    'wall_time': 136.42814469337463,\n",
       "    'tps': 10.569666568734243}],\n",
       "  'warmup_time': 350,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 8, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'deployment': 'benchmarkmodel'},\n",
       " {'benchmarks': {},\n",
       "  'warmup_time': 0,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 12, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'error': \"Failed to benchmark. ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\"},\n",
       " {'benchmarks': {},\n",
       "  'warmup_time': 600,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 16, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'error': 'Cut off start time limit reached.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalavai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
