{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark a Model\n",
    "\n",
    "A Test harness for benchmarking architectures with different deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_library.cards.architectures import *\n",
    "from model_library.cards.models import *\n",
    "from model_library.cards.deployments import *\n",
    "from model_library.models import *\n",
    "from model_library.benchmark_apis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = LLAMA_CPP\n",
    "MODEL = LLAMA_CPP_LLAMA_2_7b_CHAT_Q2_GGUF\n",
    "DEPLOYMENT = MINIMAL_CPU_DEPLOYMENT\n",
    "USER = UserInformation(\n",
    "    id=\"benchmark\",\n",
    "    namespace=\"benchmark\",\n",
    "    API_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': '/model/llama-2-7b-chat.Q2_K.gguf',\n",
       " 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF',\n",
       " 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf',\n",
       " 'volume_name': 'awesome-model-storage',\n",
       " 'pvc_name': 'awesome-model-pvc'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name = \"benchmarkmodel\"\n",
    "\n",
    "template = ModelDeploymentTemplateCard(\n",
    "    id=\"1\",\n",
    "    name=\"\",\n",
    "    description=\"\",\n",
    "    model_card=MODEL,\n",
    "    deployment_card=DEPLOYMENT,\n",
    "    params={\n",
    "        \"deployment_name\": deployment_name\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy_generic_model(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployments = list_deployments(namespace=USER.namespace)\n",
    "#deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployments = list_deployments(namespace=USER.namespace)\n",
    "#deployments\n",
    "#deployment = deployments[deployment_name]\n",
    "#port = max(deployment[\"ports\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from model_library.benchmark_apis import benchmark_model\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 09:54:50,998 - INFO - Deploying the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 09:54:56,726 - INFO - Model deployed.\n",
      "2024-01-16 09:54:56,728 - INFO - Waiting for the live service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 380/600 [06:22<03:41,  1.01s/it]\n",
      "2024-01-16 10:01:19,290 - INFO - Service is live. Warmup time: 380 seconds.\n",
      "2024-01-16 10:01:19,291 - INFO - Running benchmarks.\n",
      "100%|██████████| 25/25 [06:31<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     type  average_response_time  average_tokens_returned  \\\n",
      "0  serial              15.665051                    57.64   \n",
      "\n",
      "   total_tokens_returned   wall_time       tps  \n",
      "0                   1441  391.702164  3.678816  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "benchmark = KubernetesBenchmark(template, undeploy=False)\n",
    "result = benchmark.benchmark_deployment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DeploymentCard(\n",
    "    id=\"4\",\n",
    "    description=\"Minimal Deployment for llama.cpp\",\n",
    "    params={\n",
    "        \"replicas\": 1,\n",
    "        \"num_cpus\": 4,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\",\n",
    "    },\n",
    ")\n",
    "\n",
    "num_cpus = [4,8,12,16]\n",
    "deployments = [\n",
    "    DeploymentCard(\n",
    "    id=\"4\",\n",
    "    description=\"Minimal Deployment for llama.cpp\",\n",
    "    params={\n",
    "        \"replicas\": 2,\n",
    "        \"num_cpus\": n,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\",\n",
    "    }\n",
    "    )\n",
    "    for n in num_cpus\n",
    "]\n",
    "\n",
    "deployment_name = \"benchmarkmodel\"\n",
    "\n",
    "templates = [\n",
    "    ModelDeploymentTemplateCard(\n",
    "        id=\"1\",\n",
    "        name=\"\",\n",
    "        description=\"\",\n",
    "        model_card=MODEL,\n",
    "        deployment_card=deployment,\n",
    "        params={\n",
    "            \"deployment_name\": deployment_name\n",
    "        }\n",
    "    )\n",
    "    for deployment in deployments\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:19,920 - INFO - Deploying the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:25,485 - INFO - Model deployed.\n",
      "2024-01-15 17:19:25,486 - INFO - Waiting for the live service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]\n",
      "2024-01-15 17:19:25,775 - INFO - Service is live. Warmup time: 0 seconds.\n",
      "2024-01-15 17:19:25,776 - INFO - Running benchmarks.\n",
      " 20%|██        | 5/25 [00:24<01:37,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:50,380 - INFO - Deploying the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 17:19:56,227 - INFO - Model deployed.\n",
      "2024-01-15 17:19:56,231 - INFO - Waiting for the live service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [10:02<00:00,  1.00s/it]\n",
      "2024-01-15 17:29:59,021 - INFO - Service is live. Warmup time: 600 seconds.\n",
      "2024-01-15 17:29:59,022 - WARNING - Service did not become live within the cutoff time.\n",
      "2024-01-15 17:29:59,022 - INFO - Cleaning up the deployment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete request successful.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for n, template in zip(num_cpus[2:], templates[2:]):\n",
    "    print(\"N: \", n)\n",
    "    benchmark = KubernetesBenchmark(template, undeploy=True)\n",
    "    result = benchmark.benchmark_deployment()\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'benchmarks': [{'type': 'serial',\n",
       "    'average_response_time': 15.530141487121583,\n",
       "    'average_tokens_returned': 57.96,\n",
       "    'total_tokens_returned': 1449,\n",
       "    'wall_time': 388.30193424224854,\n",
       "    'tps': 3.7316321970624475}],\n",
       "  'warmup_time': 360,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 4, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'deployment': 'benchmarkmodel'},\n",
       " {'benchmarks': [{'type': 'serial',\n",
       "    'average_response_time': 5.455150327682495,\n",
       "    'average_tokens_returned': 57.68,\n",
       "    'total_tokens_returned': 1442,\n",
       "    'wall_time': 136.42814469337463,\n",
       "    'tps': 10.569666568734243}],\n",
       "  'warmup_time': 350,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 8, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'deployment': 'benchmarkmodel'},\n",
       " {'benchmarks': {},\n",
       "  'warmup_time': 0,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 12, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'error': \"Failed to benchmark. ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\"},\n",
       " {'benchmarks': {},\n",
       "  'warmup_time': 600,\n",
       "  'template': ModelDeploymentTemplateCard(id='1', name='', description='', model_card=ModelCard(id='3', architecture=ArchitectureCard(id='cd993f75-2601-4633-b510-60dbd954bfb2', deployment_yaml='llama_cpp_python.yaml', name='Llama.cpp (python)', description='The python llama.cpp engine', tags={'cpu': True, 'gpu': True}, health_endpoint='v1/models'), params={'model_path': '/model/llama-2-7b-chat.Q2_K.gguf', 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF', 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf', 'volume_name': 'awesome-model-storage', 'pvc_name': 'awesome-model-pvc'}), benchmarks=None, deployment_card=DeploymentCard(id='4', params={'replicas': 1, 'num_cpus': 16, 'ram_memory': '12Gi', 'pvc_storage_request': '24Gi'}), viable_deployment_cards=[], params={'deployment_name': 'benchmarkmodel'}),\n",
       "  'error': 'Cut off start time limit reached.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalavai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
