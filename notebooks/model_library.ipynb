{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/miniconda3/envs/kalavai/lib/python3.11/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_card\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/adam/miniconda3/envs/kalavai/lib/python3.11/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_deployment_template\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model_library.models import *   \n",
    "from model_library import ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARCHITECTURES[0].check_health(\n",
    "    port = 30767,\n",
    "    url = \"178.62.13.8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/v1/models'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARCHITECTURES[0].health_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "architrecture_card = ArchitectureCard(\n",
    "    name=\"llama.cpp\",\n",
    "    id = \"1\",\n",
    "    description=\"The python llama.cpp engine\",\n",
    "    tags={\"cpu\":True, \"gpu\":True},\n",
    "    deployment_yaml = \"deployment.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "user = UserInformation(\n",
    "    id=\"2\",\n",
    "    API_key= \"XXXX-XXXX-XXXX-XXXX\",\n",
    "    namespace = \"adam\",\n",
    ")\n",
    "\n",
    "\n",
    "model =ModelCard(\n",
    "    id = \"3\",\n",
    "    architecture= architrecture_card,\n",
    "    params= {\n",
    "        \"model_path\": \"/model/llama-2-7b-chat.Q2_K.gguf\",\n",
    "        \"hf_repo_id\": \"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        \"hf_filename\": \"llama-2-7b-chat.Q2_K.gguf\",\n",
    "        \"volume_name\": \"awesome-model-storage\",\n",
    "        \"pvc_name\": \"awesome-model-pvc\"\n",
    "    }   \n",
    ")\n",
    "    \n",
    "\n",
    "deployment = DeploymentCard(\n",
    "    id =  \"4\",\n",
    "    description = \"Minimal Deployment for llama.cpp\",\n",
    "    params = {\n",
    "        \"replicas\": 1,\n",
    "        \"num_cpus\": 4,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\"\n",
    "    }\n",
    ")\n",
    "\n",
    "model_deployment_template = ModelDeploymentTemplateCard(\n",
    "    id= \"1\",\n",
    "    model_card=model,\n",
    "    deployment_card=deployment,\n",
    "    benchmarks=[]\n",
    ")\n",
    "\n",
    "model_deployment = ModelDeploymentCard(\n",
    "    id=\"0\",\n",
    "    model_deployment_template=model_deployment_template,\n",
    "    user_information=user,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML string written to compiled.yaml\n"
     ]
    }
   ],
   "source": [
    "architrecture_card = ArchitectureCard(\n",
    "    name=\"llama.cpp\",\n",
    "    id = \"1\",\n",
    "    description=\"The python llama.cpp engine\", \n",
    "    tags={\"cpu\":True, \"gpu\":True},\n",
    "    deployment_yaml = \"llama_cpp_python.yaml\" #\n",
    ") \n",
    "\n",
    "\n",
    "user = UserInformation(\n",
    "    id=\"2\",\n",
    "    API_key= \"XXXX-XXXX-XXXX-XXXX\",\n",
    "    namespace = \"adam2\",\n",
    ")\n",
    "\n",
    "model =ModelCard(\n",
    "    id = \"3\",\n",
    "    architecture= architrecture_card,\n",
    "    params= {\n",
    "        \"model_path\": \"/model/llama-2-7b-chat.Q2_K.gguf\",\n",
    "        \"hf_repo_id\": \"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        \"hf_filename\": \"llama-2-7b-chat.Q2_K.gguf\",\n",
    "        \"volume_name\": \"awesome-model-storage\",\n",
    "        \"pvc_name\": \"awesome-model-pvc\"\n",
    "    }   \n",
    ")\n",
    "    \n",
    "\n",
    "deployment = DeploymentCard(\n",
    "    id =  \"4\",\n",
    "    description = \"Minimal Deployment for llama.cpp\",\n",
    "    params = {\n",
    "        \"deployment_name\": \"llamma-cpp\",\n",
    "        \"replicas\": 1,\n",
    "        \"num_cpus\": 4,\n",
    "        \"ram_memory\": \"12Gi\",\n",
    "        \"pvc_storage_request\": \"24Gi\"\n",
    "    }\n",
    ")\n",
    "\n",
    "model_deployment_template = ModelDeploymentTemplateCard(\n",
    "    id= \"1\",\n",
    "    model_card=model,\n",
    "    deployment_card=deployment,\n",
    "    benchmarks=[]\n",
    ")\n",
    "\n",
    "model_deployment = ModelDeploymentCard(\n",
    "    id=\"0\",\n",
    "    model_deployment_template=model_deployment_template,\n",
    "    user_information=user,\n",
    ")\n",
    "\n",
    "\n",
    "from model_library.utils import create_deployment_yaml\n",
    "\n",
    "\n",
    "deployment = create_deployment_yaml(\n",
    "    model_deployment.extract_values().values,\n",
    "    model_deployment.extract_values().yaml\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"compiled.yaml\"  # Replace with your desired file path\n",
    "\n",
    "# Writing the string to a file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(deployment)\n",
    "\n",
    "print(f\"YAML string written to {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def deploy_generic_model(config):\n",
    "    # Read the configuration from the file\n",
    "\n",
    "    # Prepare the request data\n",
    "    data = {\n",
    "        \"config\": config\n",
    "    }\n",
    "\n",
    "    # Define the URL and headers\n",
    "    url = \"http://0.0.0.0:8000/v1/deploy_generic_model\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Deployment successful.\")\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(f\"Deployment failed with status code {response.status_code}.\")\n",
    "        print(response.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sparse Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment successful.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "architecture_card = ArchitectureCard(\n",
    "    name=\"deepsparse\",\n",
    "    id=\"1\",\n",
    "    description=\"Deepsparse Server Deployment\",\n",
    "    tags={\"cpu\": True},\n",
    "    deployment_yaml=\"deepsparse_deployment.yaml\"\n",
    ")\n",
    "\n",
    "model = ModelCard(\n",
    "    id=\"3\",\n",
    "    architecture=architecture_card,\n",
    "    params={\n",
    "        \"task\": \"text-generations\",\n",
    "        \"model_id\": \"zoo:llama2-7b-llama2_chat_llama2_pretrain-base_quantized\",\n",
    "    }\n",
    ")\n",
    "\n",
    "deployment = DeploymentCard(\n",
    "    id=\"4\",\n",
    "    description=\"Deepsparse Model Deployment\",\n",
    "    params={\n",
    "        \"deployment_name\":\"deepsparse\",\n",
    "        \"replicas\": 2,\n",
    "        \"num_cores\": 4,\n",
    "        \"ram_memory\": \"8Gi\",\n",
    "        \"ephemeral_memory\": \"2Gi\"\n",
    "    }\n",
    ")\n",
    "\n",
    "model_deployment_template = ModelDeploymentTemplateCard(\n",
    "    id=\"5\",\n",
    "    model_card=model,\n",
    "    deployment_card=deployment,\n",
    "    benchmarks=[]\n",
    ")\n",
    "\n",
    "model_deployment = ModelDeploymentCard(\n",
    "    id=\"0\",\n",
    "    model_deployment_template=model_deployment_template,\n",
    "    user_information=user\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ds_deployment = create_deployment_yaml(\n",
    "    model_deployment.extract_values().values,\n",
    "    model_deployment.extract_values().yaml\n",
    "\n",
    ")\n",
    "\n",
    "user = UserInformation(\n",
    "    id=\"2\",\n",
    "    API_key=\"YYYY-YYYY-YYYY-YYYY\",\n",
    "    namespace=\"adam3\"\n",
    ")\n",
    "\n",
    "deploy_generic_model(ds_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment successful.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "architecture_card = ArchitectureCard(\n",
    "    id=str(uuid.uuid4()),\n",
    "    deployment_yaml=\"deepsparse_deployment2.yaml\",\n",
    "    name=\"deepsparse\",\n",
    "    description=\"Deepsparse Deployment for Chat Model\",\n",
    "    tags={\"type\": \"deepsparse\", \"version\": \"v1.6.1\"}\n",
    ")\n",
    "\n",
    "\n",
    "model_card = ModelCard(\n",
    "    id=str(uuid.uuid4()),\n",
    "    architecture=architecture_card,\n",
    "    params={\n",
    "        \"image\": \"ghcr.io/neuralmagic/deepsparse:v1.6.1\",\n",
    "        \"sparsezoo_models_path\": \"/home/user/llama\",\n",
    "        \"container_port\": 5543,\n",
    "        \"task\": \"chat\",\n",
    "        \"model_path\": \"zoo:llama2-7b-llama2_chat_llama2_pretrain-base_quantized\",\n",
    "        \"batch_size\": 1,\n",
    "        \"num_workers\": 4,\n",
    "        \"num_cores\": 4\n",
    "    }\n",
    ")\n",
    "\n",
    "deployment_card = DeploymentCard(\n",
    "    id=str(uuid.uuid4()),\n",
    "    params={\n",
    "        \"replicas\": 1,\n",
    "        \"cpu_limits\": \"6\",\n",
    "        \"memory_limits\": \"18Gi\",\n",
    "        \"cpu_requests\": \"6\",\n",
    "        \"memory_requests\": \"18Gi\",\n",
    "        \"volume_mount_name\": \"deepsparse-model-storage\",\n",
    "        \"mount_path\": \"/home/user\",\n",
    "        \"volume_name\": \"deepsparse-model-storage\",\n",
    "        \"pvc_name\": \"deepsparse-model-pvc\",\n",
    "        \"service_name\": \"deepsparse-model\",\n",
    "        \"port_name\": \"https\",\n",
    "        \"port\": 5543,\n",
    "        \"pvc_storage_request\": \"30Gi\",\n",
    "    }\n",
    ")\n",
    "\n",
    "model_deployment_template = ModelDeploymentTemplateCard(\n",
    "    id=str(uuid.uuid4()),\n",
    "    name=\"Deepsparse Model Deployment Template\",\n",
    "    description=\"Template for deploying Deepsparse model\",\n",
    "    model_card=model_card,\n",
    "    deployment_card=deployment_card,\n",
    "    benchmarks=[],\n",
    "    params={\n",
    "        \"deployment_name\":\"deepsparse-llm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "user = UserInformation(\n",
    "    id=\"2\",\n",
    "    API_key=\"YYYY-YYYY-YYYY-YYYY\",\n",
    "    namespace=\"adam3\"\n",
    ")\n",
    "\n",
    "model_deployment = ModelDeploymentCard(\n",
    "    id=\"0\",\n",
    "    model_deployment_template=model_deployment_template,\n",
    "    user_information=user\n",
    ")\n",
    "from model_library.utils import create_deployment_yaml\n",
    "ds_deployment = create_deployment_yaml(\n",
    "    model_deployment.extract_values().values,\n",
    "    model_deployment.extract_values().yaml\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "deploy_generic_model(ds_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'namespace': 'adam2',\n",
       " 'API_key': 'XXXX-XXXX-XXXX-XXXX',\n",
       " 'model_path': '/model/llama-2-7b-chat.Q2_K.gguf',\n",
       " 'hf_repo_id': 'TheBloke/Llama-2-7B-Chat-GGUF',\n",
       " 'hf_filename': 'llama-2-7b-chat.Q2_K.gguf',\n",
       " 'volume_name': 'awesome-model-storage',\n",
       " 'pvc_name': 'awesome-model-pvc',\n",
       " 'deployment_name': 'llamma-cpp',\n",
       " 'replicas': 1,\n",
       " 'num_cpus': 4,\n",
       " 'ram_memory': '12Gi',\n",
       " 'pvc_storage_request': '24Gi'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalavai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
